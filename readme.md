# DHAF-Net

Official PyTorch implementation of â€œDHAF-Net: Decoupled and Hierarchical Attention Fusion Network for RGB-Infrared Object Detectionâ€

[Paper]()

ğŸ”’ The core implementation folder **`ultralytics/`** will be released after the paper is accepted.

## Abstract

èåˆå¯è§å…‰ï¼ˆRGBï¼‰ä¸çº¢å¤–ï¼ˆIRï¼‰å›¾åƒæ˜¯å®ç°å…¨å¤©å€™é²æ£’ç›®æ ‡æ£€æµ‹çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€èåˆæ–¹æ³•åœ¨ç‰¹å¾äº¤äº’è¿‡ç¨‹ä¸­å¸¸å¸¸é¢ä¸´æ¨¡æ€ä¸å¹³è¡¡ã€ä¿¡æ¯å†—ä½™ã€æ¨¡æ€å¹²æ‰°ä»¥åŠæ¨¡æ€ç‰¹æœ‰ä¿¡æ¯è¢«æŠ‘åˆ¶ç­‰é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£è€¦ä¸åˆ†å±‚æ³¨æ„åŠ›èåˆç½‘ç»œï¼ˆDecoupled and Hierarchical Attention Fusion Network, DHAF-Netï¼‰ï¼Œé‡æ–°å®šä¹‰äº†å¤šæ¨¡æ€ç‰¹å¾èåˆçš„èŒƒå¼ã€‚å…·ä½“è€Œè¨€ï¼ŒDHAF-Netå¼•å…¥äº†ä¸€ä¸ªç‰¹å¾è§£è€¦æ¡†æ¶ï¼Œå°†è·¨æ¨¡æ€ä¿¡æ¯æ˜¾å¼åœ°åˆ†è§£ä¸ºäº’è¡¥çš„â€œæ¨¡æ€ç‰¹æ€§ï¼ˆSpecificityï¼‰â€ä¸â€œæ¨¡æ€å…±æ€§ï¼ˆCommonalityï¼‰â€ä¸¤ä¸ªåˆ†æ”¯ï¼Œä»è€Œæœ€å¤§åŒ–åœ°ä¿ç•™å¹¶åˆ©ç”¨å„æ¨¡æ€ä¿¡æ¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡è®¾è®¡äº†è§£è€¦ä¸åˆ†å±‚æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆDHAF moduleï¼‰ï¼Œåœ¨å¤šå°ºåº¦å±‚é¢å®ç°ç‰¹å¾çš„ç²¾ç»†åŒ–å¢å¼ºä¸èåˆã€‚è¯¥æ¨¡å—é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•è·å¹¶å¼ºåŒ–ç‰¹æ€§æµå†…éƒ¨çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›å…±æ€§æµä¹‹é—´çš„å¯¹ç§°äº¤äº’ä¸è¯­ä¹‰å¯¹é½ã€‚æœ€åï¼Œå¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„é—¨æ§åŠ æƒæœºåˆ¶ï¼Œå¯¹å¤šè·¯å¢å¼ºåçš„ç‰¹å¾æµè¿›è¡Œè‡ªé€‚åº”åŠ æƒï¼Œæœ‰æ•ˆç¼“è§£æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨LLVIPã€M^3^FDç­‰å…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒDHAF-Netåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œè¾¾åˆ°äº†å½“å‰æœ€ä¼˜ï¼ˆstate-of-the-artï¼‰æ°´å¹³ã€‚è¿™å……åˆ†éªŒè¯äº†æ‰€æå‡ºçš„è§£è€¦ä¸åˆ†å±‚èåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹å»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚

## Overview

![model architecture](./model_architecture.png)

## Environment Setup

**1. Create a virtual environment**

```bash
conda create -n dhaf python=3.8
conda activate dhaf
```

**2. Install PyTorch**

```bash
conda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=12.1 -c pytorch -c nvidia
```

**3. Install YOLOv8 dependencies**

Copy the ultralytics folder and the pyproject.toml file to your project root directory, then run:

```bash
pip install -e .
```

## Code Modifications

DHAF-Net is implemented based on the Ultralytics YOLOv8 framework with the following modifications:

- Data loading
	- ultralytics.data.base.BaseDataset.get_image_and_label
	- ultralytics.data.utils.img2label_paths
- Model initialization
	- ultralytics.nn.tasks.DetectionModel.\_\_init\_\_
- Custom modules
	- ultralytics/nn/modules/conv.py
	- ultralytics/nn/modules/block.py
- Othersâ€¦


## Dataset & Weights

Please convert datasets to YOLO format and organize them as follows:

```
FLIR-aligned / LLVIP / M3FD / ...
    â”‚
    â”œâ”€imageIR
    â”‚    â”œâ”€test
    â”‚    â””â”€train
    â”œâ”€imageRGB
    â”‚    â”œâ”€test
    â”‚    â””â”€train
    â””â”€labels
         â”œâ”€test
         â”œâ”€train
         â””â”€classes.txxt
```

Dataset & weights download link: 

- [Baidu drive](https://pan.baidu.com/s/1LgY7_Xs86yyOJX_olyyikg?pwd=dhaf)
- [Google drive]()

## Training

To start training:

```bash
python train.py
```

You can modify training configurations (dataset path, model architecture, hyperparameters) in the YAML or configuration files.

**Evaluation & Prediction**

```
python validate.py
python predict.py
```

## Results

<table border="1" cellpadding="5" cellspacing="0" style="text-align: center;">
    <thead>
        <tr>
            <th rowspan="2">Methods</th>
            <th rowspan="2">Pub.</th>
            <th rowspan="2">Modality</th>
            <th rowspan="2">Backbone</th>
            <th colspan="3">FLIR-Aligned</th>
            <th colspan="3">LLVIP</th>
        </tr>
        <tr>
            <th>AP50</th>
            <th>AP75</th>
            <th>mAP</th>
            <th>AP50</th>
            <th>AP75</th>
            <th>mAP</th>
        </tr>
    </thead>
    <tbody>
        <!-- ç¤ºä¾‹æ•°æ®è¡Œ - æ‚¨å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šè¡Œ -->
        <tr>
            <td>Faster R-CNN</td>
            <td>15</td>
            <td>IR</td>
            <td>ResNet50</td>
            <td>73.4</td>
            <td>34.2</td>
            <td>37.9</td>
            <td>92.6</td>
            <td>48.8</td>
            <td>50.7</td>
        </tr>
        <tr>
            <td>YOLOv5</td>
            <td>20</td>
            <td>IR</td>
            <td>CSPDarknet53v5</td>
            <td>73.9</td>
            <td>35.7</td>
            <td>39.5</td>
            <td>94.6</td>
            <td>72.2</td>
            <td>61.9</td>
        </tr>
        <tr>
            <td>YOLOv8</td>
            <td>23</td>
            <td>IR</td>
            <td>CSPDarknet53v8</td>
            <td>72.9</td>
            <td>34.8</td>
            <td>38.3</td>
            <td>95.2</td>
            <td>72.5</td>
            <td>62.1</td>
        </tr>
        <tr>
            <td>Faster R-CNN</td>
            <td>15</td>
            <td>RGB</td>
            <td>ResNet50</td>
            <td>65.0</td>
            <td>22.8</td>
            <td>30.2</td>
            <td>88.8</td>
            <td>45.7</td>
            <td>47.5</td>
        </tr>
        <tr>
            <td>YOLOv5</td>
            <td>20</td>
            <td>RGB</td>
            <td>CSPDarknet53v5</td>
            <td>67.8</td>
            <td>25.9</td>
            <td>31.8</td>
            <td>90.8</td>
            <td>51.9</td>
            <td>50.0</td>
        </tr>
        <tr>
            <td>YOLOv8</td>
            <td>23</td>
            <td>RGB</td>
            <td>CSPDarknet53v8</td>
            <td>66.3</td>
            <td>25.0</td>
            <td>28.2</td>
            <td>91.9</td>
            <td>53.0</td>
            <td>54.0</td>
        </tr>
        <tr>
            <td>GAFF <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Guided_Attentive_Feature_Fusion_for_Multispectral_Pedestrian_Detection_WACV_2021_paper.pdf">[47]</a></td>
            <td>WWACV'21</td>
            <td>IR+RGB</td>
            <td>Resnet18</td>
            <td>72.9</td>
            <td>32.9</td>
            <td>37.5</td>
            <td>94.0</td>
            <td>60.2</td>
            <td>55.8</td>
        </tr>
        <tr>
            <td>ProbEn <a href="https://arxiv.org/pdf/2104.02904">[48]</a></td>
            <td>ECCV'22</td>
            <td>IR+RGB</td>
            <td>Resnet50</td>
            <td>75.5</td>
            <td>31.8</td>
            <td>37.9</td>
            <td>93.4</td>
            <td>50.2</td>
            <td>51.5</td>
        </tr>
        <tr>
            <td>CSAA <a href="https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Cao_Multimodal_Object_Detection_by_Channel_Switching_and_Spatial_Attention_CVPRW_2023_paper.pdf">[49]</a></td>
            <td>CVPR'23</td>
            <td>IR+RGB</td>
            <td>Resnet50</td>
            <td>79.2</td>
            <td>37.4</td>
            <td>41.3</td>
            <td>94.3</td>
            <td>66.6</td>
            <td>59.2</td>
        </tr>
        <tr>
            <td>CrossFormer <a href="https://www.sciencedirect.com/science/article/abs/pii/S016786552400045X">[50]</a></td>
            <td>PRL'24</td>
            <td>IR+RGB</td>
            <td>Resnet50</td>
            <td>79.3</td>
            <td>38.5</td>
            <td>42.1</td>
            <td>97.4</td>
            <td>75.4</td>
            <td>65.1</td>
        </tr>
        <tr>
            <td>RSDet <a href="https://arxiv.org/pdf/2401.10731">[51]</a></td>
            <td>24</td>
            <td>IR+RGB</td>
            <td>Resnet50</td>
            <td>83.9</td>
            <td>40.1</td>
            <td>43.8</td>
            <td>95.8</td>
            <td>70.4</td>
            <td>61.3</td>
        </tr>
        <tr>
            <td>Fusion-DETR <a href="https://ieeexplore.ieee.org/abstract/document/10929712/">[52]</a></td>
            <td>25</td>
            <td>IR+RGB</td>
            <td>Resnet101</td>
            <td>81.5</td>
            <td>-</td>
            <td>44.3</td>
            <td>96.4</td>
            <td>-</td>
            <td>64.6</td>
        </tr>
        <tr>
            <td>CFT <a href="https://arxiv.org/pdf/2111.00273">[53]</a></td>
            <td>21</td>
            <td>IR+RGB</td>
            <td>CSPDarknet53v5</td>
            <td>78.7</td>
            <td>35.5</td>
            <td>40.2</td>
            <td>97.5</td>
            <td>72.9</td>
            <td>63.6</td>
        </tr>
        <tr>
            <td>YOLO-MS <a href="https://ieeexplore.ieee.org/abstract/document/10021826">[54]</a></td>
            <td>TCDS'23</td>
            <td>IR+RGB</td>
            <td>CSPDarknet53v5</td>
            <td>75.2</td>
            <td>-</td>
            <td>38.3</td>
            <td>94.9</td>
            <td>-</td>
            <td>60.2</td>
        </tr>
        <tr>
            <td>ICAFusion <a href="https://www.sciencedirect.com/science/article/pii/S0031320323006118">[55]</a></td>
            <td>PR'24</td>
            <td>IR+RGB</td>
            <td>CSPDarknet53v5</td>
            <td>79.2</td>
            <td>36.9</td>
            <td>41.4</td>
            <td>95.2</td>
            <td>-</td>
            <td>60.1</td>
        </tr>
        <tr>
            <td>LRAF-Net <a href="https://ieeexplore.ieee.org/abstract/document/10144688">[56]</a></td>
            <td>TNNLS'24</td>
            <td>IR+RGB</td>
            <td>CSPDarknet53v5</td>
            <td>80.5</td>
            <td>-</td>
            <td>42.8</td>
            <td>97.9</td>
            <td>-</td>
            <td>66.3</td>
        </tr>
        <tr>
            <td rowspan="2">Fusion-Mamba <a href="https://ieeexplore.ieee.org/abstract/document/11124513">[57]</a></td>
            <td rowspan="2">TMM'25</td>
            <td rowspan="2">IR+RGB</td>
            <td>CSPDarknet53v5</td>
            <td>84.3</td>
            <td>-</td>
            <td>44.4</td>
            <td>96.8</td>
            <td>-</td>
            <td>62.8</td>
        </tr>
        <tr>
            <td>CSPDarknet53v8</td>
            <td>84.9</td>
            <td>45.9</td>
            <td>47.0</td>
            <td>97.0</td>
            <td>72.2</td>
            <td>64.3</td>
        </tr>
        <tr>
            <td>DHAF-Net (ours)</td>
            <td>-</td>
            <td>IR+RGB</td>
            <td>CSPDarknet53v8</td>
            <td>82.1</td>
            <td>48.5</td>
            <td>48.1</td>
            <td>97.7</td>
            <td>75.7</td>
            <td>67.4</td>
        </tr>
        <!-- å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šè¡Œ -->
    </tbody>
</table>
Please refer to the `./runs/detect` directory for training results.

## Citation

If you find this work useful in your research, please consider citing our paper:

```

```

## Acknowledgements

This project is built upon the excellent [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics) framework. We gratefully acknowledge their open-source contributions.